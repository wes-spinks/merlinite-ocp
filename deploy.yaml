kind: Pod
apiVersion: v1
metadata:
  namespace: cia-ocai
  labels:
    app: hf-text-generation-inference-server
spec:
  restartPolicy: Always
  serviceAccountName: default
  enableServiceLinks: true
  affinity: {}
  terminationGracePeriodSeconds: 120
  containers:
    - resources:
        limits:
          cpu: '2'
          memory: 8Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 8Gi
          nvidia.com/gpu: '1'
      readinessProbe:
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        timeoutSeconds: 55
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 3
      terminationMessagePath: /dev/termination-log
      name: server
      livenessProbe:
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        timeoutSeconds: 8
        periodSeconds: 100
        successThreshold: 1
        failureThreshold: 3
      env:
        - name: MODEL_ID
          value: ibm/merlinite-7b
        - name: MAX_INPUT_LENGTH
          value: '1024'
        - name: MAX_TOTAL_TOKENS
          value: '2048'
        - name: HUGGINGFACE_HUB_CACHE
          value: /models-cache
        - name: QUANTIZE
          value: bitsandbytes
        - name: NUMBA_CACHE_DIR
          value: /tmp
        - name: REVISION
          value: main
        - name: PORT
          value: '3000'
        - name: HOST
          value: 0.0.0.0
      securityContext:
        capabilities:
          drop:
            - ALL
        runAsNonRoot: true
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault
      ports:
        - name: http
          containerPort: 3000
          protocol: TCP
      imagePullPolicy: IfNotPresent
      startupProbe:
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        timeoutSeconds: 600
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 24
      volumeMounts:
        - name: modelcache
          mountPath: /models-cache
        - name: shm
          mountPath: /dev/shm
      terminationMessagePolicy: File
      image: 'ghcr.io/huggingface/text-generation-inference:1.2.0'
  serviceAccount: default
  volumes:
    - name: modelcache
      persistentVolumeClaim:
        claimName: modelcache
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
  dnsPolicy: ClusterFirst
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: PreferNoSchedule
    - key: node.kubernetes.io/not-ready
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 300
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 300
    - key: node.kubernetes.io/memory-pressure
      operator: Exists
      effect: NoSchedule
